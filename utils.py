import pandas as pd
import numpy as np
import os
import time
import logging
import json
from datetime import datetime, timedelta
from pathlib import Path
import google.generativeai as genai
import asyncio

# Import async processor
try:
    from async_processor import process_data_async
    ASYNC_AVAILABLE = True
except ImportError as e:
    ASYNC_AVAILABLE = False
    print(f"‚ö†Ô∏è Async processing kh√¥ng kh·∫£ d·ª•ng: {e}")

# Import config
from config import (
    MAX_OUTPUT_TOKENS,
    TEMPERATURE, 
    TOP_P,
    TOP_K,
    MAX_RETRIES,
    RETRY_DELAY,
    REQUEST_DELAY,
    SUPPORTED_FORMATS,
    LOG_FILE,
    LOG_LEVEL,
    BATCH_SIZE,
    ENABLE_BATCH_PROCESSING,
    ENABLE_PARALLEL_PROCESSING,
    ENABLE_ASYNC_PROCESSING,
    MAX_CONCURRENT_THREADS,
    THREAD_BATCH_SIZE,
    RATE_LIMIT_DELAY,
    MAX_RETRIES_PER_THREAD,
    THREAD_TIMEOUT,
    CIRCUIT_BREAKER_THRESHOLD,
    GEMINI_API_KEY,
    GEMINI_MODEL_NAME,
    OPENAI_API_KEY,
    OPENAI_MODEL_NAME
)

def check_openai_availability():
    """Ki·ªÉm tra OpenAI availability ƒë·ªông"""
    try:
        from openai import OpenAI
        return True, OpenAI
    except ImportError:
        try:
            import openai
            if hasattr(openai, 'OpenAI'):
                return True, openai.OpenAI
            else:
                return False, None
        except ImportError:
            return False, None

# Ki·ªÉm tra ban ƒë·∫ßu
OPENAI_AVAILABLE, OpenAI = check_openai_availability()
from tqdm import tqdm
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
import traceback

def setup_logging():
    """Thi·∫øt l·∫≠p logging cho ·ª©ng d·ª•ng"""
    logging.basicConfig(
        level=getattr(logging, LOG_LEVEL),
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(LOG_FILE, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

def initialize_gemini(api_key, model_name, fine_tuned_model_info=None):
    """Kh·ªüi t·∫°o Gemini API v·ªõi c·∫•u h√¨nh (h·ªó tr·ª£ fine-tuned models)"""
    try:
        genai.configure(api_key=api_key)
        
        # N·∫øu l√† fine-tuned model, s·ª≠ d·ª•ng model_id
        if fine_tuned_model_info and fine_tuned_model_info.get('model_id'):
            actual_model_name = fine_tuned_model_info['model_id']
            logger.info(f"üéØ S·ª≠ d·ª•ng fine-tuned model: {actual_model_name}")
        else:
            actual_model_name = model_name
            logger.info(f"ü§ñ S·ª≠ d·ª•ng standard model: {actual_model_name}")
        
        model = genai.GenerativeModel(
            model_name=actual_model_name,
            generation_config={
                "max_output_tokens": MAX_OUTPUT_TOKENS,
                "temperature": TEMPERATURE,
                "top_p": TOP_P,
                "top_k": TOP_K,
            }
        )
        
        # Attach fine-tuned model info n·∫øu c√≥
        if fine_tuned_model_info:
            model._fine_tuned_info = fine_tuned_model_info
        
        # Attach API provider info
        model._api_provider = 'gemini'
        model._api_key = api_key
        model._model_name = actual_model_name
        
        logger.info(f"‚úÖ ƒê√£ kh·ªüi t·∫°o Gemini model: {actual_model_name}")
        return model
    except Exception as e:
        logger.error(f"‚ùå L·ªói kh·ªüi t·∫°o Gemini: {str(e)}")
        return None

def initialize_openai(api_key, model_name):
    """Kh·ªüi t·∫°o OpenAI API v·ªõi c·∫•u h√¨nh"""
    # Ki·ªÉm tra l·∫°i ƒë·ªông
    available, openai_class = check_openai_availability()
    if not available or openai_class is None:
        raise ImportError("OpenAI package kh√¥ng c√≥ s·∫µn ho·∫∑c version kh√¥ng t∆∞∆°ng th√≠ch. Vui l√≤ng c√†i ƒë·∫∑t: pip install openai>=1.0.0")
    
    try:
        client = openai_class(api_key=api_key)
        
        # Test connection
        test_response = client.models.list()
        
        # Create a wrapper object that behaves like Gemini model
        class OpenAIModelWrapper:
            def __init__(self, client, model_name):
                self.client = client
                self.model_name = model_name
                self._api_provider = 'openai'
                self._fine_tuned_info = None
                self._api_key = client.api_key
                self._model_name = model_name
            
            def generate_content(self, prompt):
                try:
                    response = self.client.chat.completions.create(
                        model=self.model_name,
                        messages=[
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=MAX_OUTPUT_TOKENS,
                        temperature=TEMPERATURE,
                        top_p=TOP_P
                    )
                    
                    if response.choices and response.choices[0].message:
                        # Create response object similar to Gemini
                        class ResponseWrapper:
                            def __init__(self, text):
                                self.text = text
                        
                        return ResponseWrapper(response.choices[0].message.content)
                    else:
                        return None
                        
                except Exception as e:
                    raise Exception(f"OpenAI API error: {str(e)}")
        
        model = OpenAIModelWrapper(client, model_name)
        logger.info(f"‚úÖ ƒê√£ kh·ªüi t·∫°o OpenAI model: {model_name}")
        return model
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói kh·ªüi t·∫°o OpenAI: {str(e)}")
        return None

def initialize_ai_model(api_provider, api_key, model_name, fine_tuned_model_info=None):
    """Kh·ªüi t·∫°o AI model d·ª±a tr√™n provider"""
    if api_provider == 'gemini':
        model = initialize_gemini(api_key, model_name, fine_tuned_model_info)
        if model:
            # Th√™m th√¥ng tin API provider cho async processing
            model._api_provider = "gemini"
        return model
    elif api_provider == 'openai':
        # Ki·ªÉm tra l·∫°i ƒë·ªông
        available, _ = check_openai_availability()
        if not available:
            logger.error("‚ùå OpenAI package kh√¥ng c√≥ s·∫µn. Vui l√≤ng c√†i ƒë·∫∑t: pip install openai>=1.0.0")
            return None
        model = initialize_openai(api_key, model_name)
        if model:
            # Th√™m th√¥ng tin API provider cho async processing
            model._api_provider = "openai"
        return model
    else:
        logger.error(f"‚ùå API provider kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {api_provider}")
        return None

def validate_file_path(file_path):
    """Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa ƒë∆∞·ªùng d·∫´n file"""
    if not file_path:
        return False, "ƒê∆∞·ªùng d·∫´n file kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng"
    
    path = Path(file_path)
    
    # Ki·ªÉm tra file c√≥ t·ªìn t·∫°i
    if not path.exists():
        return False, f"File kh√¥ng t·ªìn t·∫°i: {file_path}"
    
    # Ki·ªÉm tra ƒë·ªãnh d·∫°ng file
    if path.suffix.lower() not in SUPPORTED_FORMATS:
        return False, f"ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. Ch·ªâ h·ªó tr·ª£: {', '.join(SUPPORTED_FORMATS)}"
    
    return True, "File h·ª£p l·ªá"

def load_data(file_path):
    """Load d·ªØ li·ªáu t·ª´ file Excel ho·∫∑c CSV"""
    try:
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path)
        elif file_ext == '.csv':
            # Th·ª≠ nhi·ªÅu encoding ƒë·ªÉ tr√°nh l·ªói
            encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']
            df = None
            for encoding in encodings:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue
            
            if df is None:
                raise Exception("Kh√¥ng th·ªÉ ƒë·ªçc file CSV v·ªõi c√°c encoding th√¥ng d·ª•ng")
        else:
            raise Exception(f"ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {file_ext}")
        
        logger.info(f"‚úÖ ƒê√£ load file: {file_path} ({len(df)} records)")
        return df
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói load file: {str(e)}")
        return None

def save_data(df, file_path):
    """L∆∞u d·ªØ li·ªáu ra file Excel ho·∫∑c CSV"""
    try:
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext in ['.xlsx', '.xls']:
            df.to_excel(file_path, index=False)
        elif file_ext == '.csv':
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
        else:
            raise Exception(f"ƒê·ªãnh d·∫°ng file output kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {file_ext}")
        
        logger.info(f"‚úÖ ƒê√£ l∆∞u file: {file_path}")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói l∆∞u file: {str(e)}")
        return False

def detect_message_column(df):
    """T·ª± ƒë·ªông ph√°t hi·ªán c·ªôt ch·ª©a message"""
    possible_columns = ['MESSAGE']
    
    for col in possible_columns:
        if col in df.columns:
            return col
    
    # N·∫øu kh√¥ng t√¨m th·∫•y, tr·∫£ v·ªÅ c·ªôt ƒë·∫ßu ti√™n c√≥ ki·ªÉu text
    for col in df.columns:
        if df[col].dtype == 'object':
            return col
    
    return None

def generate_output_filename(input_file):
    """T·∫°o t√™n file output d·ª±a tr√™n file input"""
    input_path = Path(input_file)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_name = f"{input_path.stem}_ai_result_{timestamp}{input_path.suffix}"
    return str(input_path.parent / output_name)

def generate_checkpoint_filename(input_file):
    """T·∫°o t√™n file checkpoint d·ª±a tr√™n file input"""
    input_path = Path(input_file)
    checkpoint_name = f"{input_path.stem}_checkpoint{input_path.suffix}"
    return str(input_path.parent / checkpoint_name)

def load_checkpoint(checkpoint_file):
    """Load checkpoint n·∫øu t·ªìn t·∫°i"""
    try:
        if os.path.exists(checkpoint_file):
            df = load_data(checkpoint_file)
            if df is not None:
                logger.info(f"üîÑ ƒê√£ load checkpoint: {checkpoint_file}")
                return df
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è L·ªói load checkpoint: {str(e)}")
    
    return None

def save_checkpoint(df, checkpoint_file):
    """L∆∞u checkpoint"""
    try:
        save_data(df, checkpoint_file)
        logger.info(f"üíæ ƒê√£ l∆∞u checkpoint: {checkpoint_file}")
    except Exception as e:
        logger.error(f"‚ùå L·ªói l∆∞u checkpoint: {str(e)}")

def process_text_with_ai(model, text, prompt, max_retries=MAX_RETRIES):
    """X·ª≠ l√Ω text v·ªõi AI model (h·ªó tr·ª£ fine-tuned models v·ªõi prompt context)"""
    for attempt in range(max_retries):
        try:
            # Ki·ªÉm tra n·∫øu l√† fine-tuned model v·ªõi prompt context
            if hasattr(model, '_fine_tuned_info') and model._fine_tuned_info:
                fine_tuned_info = model._fine_tuned_info
                
                # N·∫øu model c√≥ prompt context, s·ª≠ d·ª•ng format ƒë√£ training
                if fine_tuned_info.get('requires_context', False):
                    prompt_context = fine_tuned_info.get('prompt_context', {})
                    context_template = prompt_context.get('template', '')
                    
                    if context_template:
                        # Apply prompt context template
                        full_prompt = apply_fine_tuned_prompt_context(
                            text, prompt, context_template
                        )
                        logger.debug(f"üéØ S·ª≠ d·ª•ng fine-tuned prompt context")
                    else:
                        full_prompt = f"{prompt}\n\nN·ªôi dung c·∫ßn x·ª≠ l√Ω:\n{text}\n\nK·∫øt qu·∫£:"
                else:
                    full_prompt = f"{prompt}\n\nN·ªôi dung c·∫ßn x·ª≠ l√Ω:\n{text}\n\nK·∫øt qu·∫£:"
            else:
                # Standard model processing
                full_prompt = f"{prompt}\n\nN·ªôi dung c·∫ßn x·ª≠ l√Ω:\n{text}\n\nK·∫øt qu·∫£:"
            
            # G·ªçi API
            response = model.generate_content(full_prompt)
            
            # Delay ƒë·ªÉ tr√°nh rate limit
            time.sleep(REQUEST_DELAY)
            
            if response and response.text:
                result = response.text.strip()
                logger.debug(f"‚úÖ X·ª≠ l√Ω th√†nh c√¥ng: {text[:50]}...")
                return result
            else:
                raise Exception("Kh√¥ng nh·∫≠n ƒë∆∞·ª£c response t·ª´ AI")
                
        except Exception as e:
            error_msg = str(e)
            
            # X·ª≠ l√Ω l·ªói rate limit (cho c·∫£ Gemini v√† OpenAI)
            if ("429" in error_msg or "quota" in error_msg.lower() or 
                "rate_limit" in error_msg.lower() or "too_many_requests" in error_msg.lower()):
                if attempt < max_retries - 1:
                    wait_time = RETRY_DELAY * (attempt + 1)  # TƒÉng d·∫ßn th·ªùi gian ch·ªù
                    logger.warning(f"‚è≥ Rate limit reached. Ch·ªù {wait_time} gi√¢y...")
                    time.sleep(wait_time)
                    continue
            
            # X·ª≠ l√Ω l·ªói kh√°c
            logger.error(f"‚ùå L·ªói x·ª≠ l√Ω text (attempt {attempt + 1}): {error_msg}")
            
            if attempt < max_retries - 1:
                time.sleep(5)  # Ch·ªù ng·∫Øn tr∆∞·ªõc khi th·ª≠ l·∫°i
                continue
    
    return f"L·ªói x·ª≠ l√Ω sau {max_retries} l·∫ßn th·ª≠"

def process_multicolumn_with_ai(model, row_data, column_names, prompt, max_retries=MAX_RETRIES):
    """X·ª≠ l√Ω nhi·ªÅu c·ªôt v·ªõi AI model"""
    try:
        # T·∫°o c·∫•u tr√∫c d·ªØ li·ªáu cho prompt
        data_structure = "\n".join([
            f"{i+1}. {col_name}: {clean_text(str(row_data.get(col_name, 'N/A')))}"
            for i, col_name in enumerate(column_names)
        ])
        
        # T·∫°o ph·∫ßn ƒë·ªãnh nghƒ©a c·ªôt cho prompt
        column_definitions = "\n".join([
            f"- C·ªôt {i+1} ({col_name}): {_get_column_description(col_name)}"
            for i, col_name in enumerate(column_names)
        ])
        
        # T·∫°o prompt ƒë·∫ßy ƒë·ªß v·ªõi ƒë·ªãnh nghƒ©a c·ªôt
        full_prompt = f"""
{prompt}

TH√îNG TIN C√ÅC C·ªòT:
{column_definitions}

D·ªÆ LI·ªÜU C·∫¶N X·ª¨ L√ù:
{data_structure}

K·∫øt qu·∫£:"""
        
        # G·ªçi h√†m x·ª≠ l√Ω AI
        return process_text_with_ai(model, "", full_prompt, max_retries)
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói x·ª≠ l√Ω multi-column: {str(e)}")
        return f"L·ªói x·ª≠ l√Ω multi-column: {str(e)}"

def _get_column_description(column_name):
    """T·∫°o m√¥ t·∫£ cho c·ªôt d·ª±a tr√™n t√™n c·ªôt"""
    return column_name

def process_batch_with_ai(model, batch_data, prompt, max_retries=MAX_RETRIES):
    """X·ª≠ l√Ω batch data v·ªõi AI model - single column mode"""
    try:
        # T·∫°o prompt cho batch processing
        batch_items = []
        for i, item in enumerate(batch_data, 1):
            cleaned_text = clean_text(str(item))
            batch_items.append(f"[{i}] {cleaned_text}")
        
        batch_content = "\n\n".join(batch_items)
        
        # T·∫°o prompt ƒë·∫ßy ƒë·ªß cho batch
        full_prompt = f"""{prompt}

H∆Ø·ªöNG D·∫™N BATCH PROCESSING:
- X·ª≠ l√Ω {len(batch_data)} m·ª•c d·ªØ li·ªáu d∆∞·ªõi ƒë√¢y
- Tr·∫£ v·ªÅ k·∫øt qu·∫£ theo th·ª© t·ª± t∆∞∆°ng ·ª©ng
- Format: [1] K·∫øt qu·∫£ 1\n[2] K·∫øt qu·∫£ 2\n[3] K·∫øt qu·∫£ 3...

D·ªÆ LI·ªÜU C·∫¶N X·ª¨ L√ù:
{batch_content}

K·∫æT QU·∫¢:"""

        # G·ªçi AI
        for attempt in range(max_retries):
            try:
                response = model.generate_content(full_prompt)
                
                # Delay ƒë·ªÉ tr√°nh rate limit  
                time.sleep(REQUEST_DELAY)
                
                if response and response.text:
                    # Parse k·∫øt qu·∫£ batch
                    results = parse_batch_results(response.text.strip(), len(batch_data))
                    logger.debug(f"‚úÖ X·ª≠ l√Ω batch th√†nh c√¥ng: {len(results)} results")
                    return results
                else:
                    raise Exception("Kh√¥ng nh·∫≠n ƒë∆∞·ª£c response t·ª´ AI")
                    
            except Exception as e:
                error_msg = str(e)
                
                # X·ª≠ l√Ω l·ªói rate limit (cho c·∫£ Gemini v√† OpenAI)
                if ("429" in error_msg or "quota" in error_msg.lower() or 
                    "rate_limit" in error_msg.lower() or "too_many_requests" in error_msg.lower()):
                    if attempt < max_retries - 1:
                        wait_time = RETRY_DELAY * (attempt + 1)
                        logger.warning(f"‚è≥ Rate limit reached. Ch·ªù {wait_time} gi√¢y...")
                        time.sleep(wait_time)
                        continue
                
                # X·ª≠ l√Ω l·ªói kh√°c
                logger.error(f"‚ùå L·ªói x·ª≠ l√Ω batch (attempt {attempt + 1}): {error_msg}")
                
                if attempt < max_retries - 1:
                    time.sleep(5)
                    continue
        
        # N·∫øu batch th·∫•t b·∫°i, fallback v·ªÅ single processing
        logger.warning("‚ö†Ô∏è Batch processing th·∫•t b·∫°i, fallback v·ªÅ single processing")
        results = []
        for item in batch_data:
            result = process_text_with_ai(model, str(item), prompt, max_retries)
            results.append(result)
        return results
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói x·ª≠ l√Ω batch: {str(e)}")
        # Fallback v·ªÅ single processing
        results = []
        for item in batch_data:
            result = process_text_with_ai(model, str(item), prompt, max_retries)
            results.append(result)
        return results

def process_multicolumn_batch_with_ai(model, batch_rows, column_names, prompt, max_retries=MAX_RETRIES):
    """X·ª≠ l√Ω batch data v·ªõi AI model - multi-column mode"""
    try:
        # T·∫°o prompt cho multi-column batch processing
        batch_items = []
        
        # T·∫°o ph·∫ßn ƒë·ªãnh nghƒ©a c·ªôt
        column_definitions = "\n".join([
            f"- C·ªôt {i+1} ({col_name}): {_get_column_description(col_name)}"
            for i, col_name in enumerate(column_names)
        ])
        
        # T·∫°o d·ªØ li·ªáu batch
        for i, row_data in enumerate(batch_rows, 1):
            data_structure = "\n".join([
                f"  {j+1}. {col_name}: {clean_text(str(row_data.get(col_name, 'N/A')))}"
                for j, col_name in enumerate(column_names)
            ])
            batch_items.append(f"[{i}]\n{data_structure}")
        
        batch_content = "\n\n".join(batch_items)
        
        # T·∫°o prompt ƒë·∫ßy ƒë·ªß cho multi-column batch
        full_prompt = f"""{prompt}

TH√îNG TIN C√ÅC C·ªòT:
{column_definitions}

H∆Ø·ªöNG D·∫™N BATCH PROCESSING:
- X·ª≠ l√Ω {len(batch_rows)} m·ª•c d·ªØ li·ªáu d∆∞·ªõi ƒë√¢y
- M·ªói m·ª•c c√≥ {len(column_names)} c·ªôt th√¥ng tin
- Tr·∫£ v·ªÅ k·∫øt qu·∫£ theo th·ª© t·ª± t∆∞∆°ng ·ª©ng
- Format: [1] K·∫øt qu·∫£ 1\n[2] K·∫øt qu·∫£ 2\n[3] K·∫øt qu·∫£ 3...

D·ªÆ LI·ªÜU C·∫¶N X·ª¨ L√ù:
{batch_content}

K·∫æT QU·∫¢:"""

        # G·ªçi AI
        for attempt in range(max_retries):
            try:
                response = model.generate_content(full_prompt)
                
                # Delay ƒë·ªÉ tr√°nh rate limit
                time.sleep(REQUEST_DELAY)
                
                if response and response.text:
                    # Parse k·∫øt qu·∫£ batch
                    results = parse_batch_results(response.text.strip(), len(batch_rows))
                    logger.debug(f"‚úÖ X·ª≠ l√Ω multi-column batch th√†nh c√¥ng: {len(results)} results")
                    return results
                else:
                    raise Exception("Kh√¥ng nh·∫≠n ƒë∆∞·ª£c response t·ª´ AI")
                    
            except Exception as e:
                error_msg = str(e)
                
                # X·ª≠ l√Ω l·ªói rate limit (cho c·∫£ Gemini v√† OpenAI)
                if ("429" in error_msg or "quota" in error_msg.lower() or 
                    "rate_limit" in error_msg.lower() or "too_many_requests" in error_msg.lower()):
                    if attempt < max_retries - 1:
                        wait_time = RETRY_DELAY * (attempt + 1)
                        logger.warning(f"‚è≥ Rate limit reached. Ch·ªù {wait_time} gi√¢y...")
                        time.sleep(wait_time)
                        continue
                
                # X·ª≠ l√Ω l·ªói kh√°c
                logger.error(f"‚ùå L·ªói x·ª≠ l√Ω multi-column batch (attempt {attempt + 1}): {error_msg}")
                
                if attempt < max_retries - 1:
                    time.sleep(5)
                    continue
        
        # N·∫øu batch th·∫•t b·∫°i, fallback v·ªÅ single processing
        logger.warning("‚ö†Ô∏è Multi-column batch processing th·∫•t b·∫°i, fallback v·ªÅ single processing")
        results = []
        for row_data in batch_rows:
            result = process_multicolumn_with_ai(model, row_data, column_names, prompt, max_retries)
            results.append(result)
        return results
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói x·ª≠ l√Ω multi-column batch: {str(e)}")
        # Fallback v·ªÅ single processing
        results = []
        for row_data in batch_rows:
            result = process_multicolumn_with_ai(model, row_data, column_names, prompt, max_retries)
            results.append(result)
        return results

def parse_batch_results(response_text, expected_count):
    """Parse k·∫øt qu·∫£ t·ª´ batch response"""
    try:
        results = []
        lines = response_text.split('\n')
        
        current_result = ""
        current_index = 0
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Ki·ªÉm tra n·∫øu l√† d√≤ng b·∫Øt ƒë·∫ßu v·ªõi [s·ªë]
            if line.startswith('[') and ']' in line:
                # L∆∞u k·∫øt qu·∫£ tr∆∞·ªõc ƒë√≥ n·∫øu c√≥
                if current_result and current_index > 0:
                    results.append(current_result.strip())
                
                # B·∫Øt ƒë·∫ßu k·∫øt qu·∫£ m·ªõi
                bracket_end = line.find(']')
                try:
                    index = int(line[1:bracket_end])
                    current_index = index
                    current_result = line[bracket_end + 1:].strip()
                except ValueError:
                    # N·∫øu kh√¥ng parse ƒë∆∞·ª£c s·ªë, coi nh∆∞ l√† n·ªôi dung
                    current_result += " " + line
            else:
                # Ti·∫øp t·ª•c n·ªôi dung c·ªßa k·∫øt qu·∫£ hi·ªán t·∫°i
                current_result += " " + line
        
        # L∆∞u k·∫øt qu·∫£ cu·ªëi c√πng
        if current_result and current_index > 0:
            results.append(current_result.strip())
        
        # ƒê·∫£m b·∫£o s·ªë l∆∞·ª£ng k·∫øt qu·∫£ ƒë√∫ng
        while len(results) < expected_count:
            results.append("L·ªói parse k·∫øt qu·∫£")
        
        # C·∫Øt b·ªõt n·∫øu c√≥ qu√° nhi·ªÅu k·∫øt qu·∫£
        results = results[:expected_count]
        
        logger.debug(f"‚úÖ Parse batch results: {len(results)}/{expected_count}")
        return results
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói parse batch results: {str(e)}")
        # Fallback: tr·∫£ v·ªÅ response nguy√™n cho t·∫•t c·∫£
        return [response_text] * expected_count

# ===========================
# PARALLEL PROCESSING FUNCTIONS
# ===========================

class CircuitBreaker:
    """Circuit breaker pattern cho parallel processing"""
    def __init__(self, threshold=CIRCUIT_BREAKER_THRESHOLD):
        self.threshold = threshold
        self.failure_count = 0
        self.last_failure_time = None
        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN
        self.reset_timeout = 60  # seconds
        self._lock = threading.Lock()
    
    def call(self, func, *args, **kwargs):
        """G·ªçi function v·ªõi circuit breaker protection"""
        with self._lock:
            if self.state == 'OPEN':
                if time.time() - self.last_failure_time > self.reset_timeout:
                    self.state = 'HALF_OPEN'
                else:
                    raise Exception("Circuit breaker is OPEN")
            
            try:
                result = func(*args, **kwargs)
                if self.state == 'HALF_OPEN':
                    self.state = 'CLOSED'
                    self.failure_count = 0
                return result
            
            except Exception as e:
                self.failure_count += 1
                self.last_failure_time = time.time()
                
                if self.failure_count >= self.threshold:
                    self.state = 'OPEN'
                    logger.warning(f"üî¥ Circuit breaker OPEN - {self.failure_count} failures")
                
                raise e

def process_parallel_batch_worker(thread_id, model, batch_data, column_names, prompt, is_multicolumn, circuit_breaker, progress_queue):
    """Worker function cho parallel processing"""
    try:
        logger.info(f"üöÄ Thread {thread_id} b·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(batch_data)} items")
        
        # Delay staggered ƒë·ªÉ tr√°nh rate limit
        time.sleep(thread_id * RATE_LIMIT_DELAY)
        
        # X·ª≠ l√Ω v·ªõi circuit breaker protection
        if is_multicolumn:
            results = circuit_breaker.call(
                process_multicolumn_batch_with_ai,
                model, batch_data, column_names, prompt, MAX_RETRIES_PER_THREAD
            )
        else:
            results = circuit_breaker.call(
                process_batch_with_ai,
                model, batch_data, prompt, MAX_RETRIES_PER_THREAD
            )
        
        # Report progress
        progress_queue.put(('success', thread_id, len(batch_data)))
        logger.info(f"‚úÖ Thread {thread_id} ho√†n th√†nh th√†nh c√¥ng")
        
        return results
        
    except Exception as e:
        error_msg = f"Thread {thread_id} l·ªói: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        logger.debug(traceback.format_exc())
        
        # Report error
        progress_queue.put(('error', thread_id, str(e)))
        
        # Fallback v·ªÅ single processing cho batch n√†y
        results = []
        try:
            if is_multicolumn:
                for row_data in batch_data:
                    result = process_multicolumn_with_ai(model, row_data, column_names, prompt, MAX_RETRIES_PER_THREAD)
                    results.append(result)
            else:
                for item in batch_data:
                    result = process_text_with_ai(model, str(item), prompt, MAX_RETRIES_PER_THREAD)
                    results.append(result)
        except Exception as fallback_error:
            logger.error(f"‚ùå Thread {thread_id} fallback c≈©ng th·∫•t b·∫°i: {str(fallback_error)}")
            results = [f"L·ªói x·ª≠ l√Ω: {str(e)}"] * len(batch_data)
        
        return results

def process_data_with_async(model, data, column_names, prompt, is_multicolumn=False):
    """X·ª≠ l√Ω d·ªØ li·ªáu v·ªõi Async Processing (thay th·∫ø parallel processing)"""
    # Import config t·∫°i runtime ƒë·ªÉ tr√°nh l·ªói circular import
    try:
        from config import ENABLE_ASYNC_PROCESSING
    except ImportError as e:
        logger.error(f"‚ùå Kh√¥ng th·ªÉ import config: {e}")
        return process_data_batch_only(model, data, column_names, prompt, is_multicolumn)
    
    if not ASYNC_AVAILABLE:
        logger.warning("‚ö†Ô∏è Async processing kh√¥ng kh·∫£ d·ª•ng, fallback v·ªÅ batch processing")
        return process_data_batch_only(model, data, column_names, prompt, is_multicolumn)
    
    if not ENABLE_ASYNC_PROCESSING:
        logger.info("üîÑ Async processing b·ªã t·∫Øt, chuy·ªÉn v·ªÅ batch processing")
        return process_data_batch_only(model, data, column_names, prompt, is_multicolumn)
    
    try:
        total_items = len(data)
        logger.info(f"üöÄ Starting async processing: {total_items} items")
        
        # Extract API info t·ª´ model - L·∫§Y TR·ª∞C TI·∫æP T·ª™ MODEL OBJECT
        api_provider = getattr(model, '_api_provider', 'gemini')
        
        # L·∫•y API key v√† model name t·ª´ model object (ƒë√£ ƒë∆∞·ª£c set khi initialize)
        if hasattr(model, '_api_key') and hasattr(model, '_model_name'):
            api_key = model._api_key
            model_name = model._model_name
            logger.info(f"üîë S·ª≠ d·ª•ng API key t·ª´ model: {api_provider} - {model_name}")
        else:
            # Kh√¥ng c√≥ API key trong model, fallback v·ªÅ batch processing
            logger.warning("‚ö†Ô∏è Model kh√¥ng c√≥ API key cho async processing, fallback v·ªÅ batch processing")
            return process_data_batch_only(model, data, column_names, prompt, is_multicolumn)
        
        # Prepare data cho async processing
        if is_multicolumn:
            # Convert DataFrame rows to dict cho multicolumn
            data_for_async = []
            for _, row in data.iterrows() if hasattr(data, 'iterrows') else enumerate(data):
                if hasattr(data, 'iterrows'):
                    data_for_async.append(row.to_dict())
                else:
                    # N·∫øu data l√† list of dicts
                    data_for_async.append(row)
        else:
            # Single column data
            if hasattr(data, 'tolist'):
                data_for_async = data.tolist()
            else:
                data_for_async = list(data)
        
        # Run async processing
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            results = loop.run_until_complete(
                process_data_async(
                    api_provider=api_provider,
                    api_key=api_key,
                    model_name=model_name,
                    data=data_for_async,
                    prompt_template=prompt,
                    is_multicolumn=is_multicolumn,
                    column_names=column_names
                )
            )
            
            logger.info(f"‚úÖ Async processing completed: {len(results)} results")
            return results
            
        finally:
            loop.close()
        
    except Exception as e:
        logger.error(f"‚ùå Async processing failed: {str(e)}")
        logger.debug(traceback.format_exc())
        logger.info("üîÑ Fallback to batch processing")
        return process_data_batch_only(model, data, column_names, prompt, is_multicolumn)

def process_data_parallel(model, data, column_names, prompt, is_multicolumn=False):
    """X·ª≠ l√Ω d·ªØ li·ªáu v·ªõi parallel processing (DEPRECATED - s·ª≠ d·ª•ng async thay th·∫ø)"""
    logger.warning("‚ö†Ô∏è process_data_parallel is deprecated, using async processing instead")
    return process_data_with_async(model, data, column_names, prompt, is_multicolumn)

def process_data_batch_only(model, data, column_names, prompt, is_multicolumn=False):
    """X·ª≠ l√Ω d·ªØ li·ªáu ch·ªâ v·ªõi batch processing (kh√¥ng parallel/async)"""
    try:
        total_items = len(data)
        batch_size = BATCH_SIZE
        all_results = []
        
        logger.info(f"üì¶ Batch processing: {total_items} items v·ªõi batch size {batch_size}")
        
        with tqdm(total=total_items, desc="üîÑ Batch Processing") as pbar:
            for i in range(0, total_items, batch_size):
                batch = data[i:i+batch_size]
                
                if is_multicolumn:
                    results = process_multicolumn_batch_with_ai(model, batch, column_names, prompt)
                else:
                    results = process_batch_with_ai(model, batch, prompt)
                
                all_results.extend(results)
                pbar.update(len(batch))
        
        return all_results
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói batch processing: {str(e)}")
        # Fallback v·ªÅ single processing
        results = []
        for item in data:
            if is_multicolumn:
                result = process_multicolumn_with_ai(model, item, column_names, prompt)
            else:
                result = process_text_with_ai(model, str(item), prompt)
            results.append(result)
        return results

def estimate_completion_time(processed, total, start_time):
    """∆Ø·ªõc t√≠nh th·ªùi gian ho√†n th√†nh"""
    if processed == 0:
        return "Kh√¥ng th·ªÉ ∆∞·ªõc t√≠nh"
    
    elapsed = time.time() - start_time
    rate = processed / elapsed  # records per second
    remaining = total - processed
    
    if rate > 0:
        eta_seconds = remaining / rate
        eta = datetime.now() + timedelta(seconds=eta_seconds)
        return f"D·ª± ki·∫øn ho√†n th√†nh: {eta.strftime('%H:%M:%S %d/%m/%Y')}"
    
    return "Kh√¥ng th·ªÉ ∆∞·ªõc t√≠nh"

def print_progress_summary(processed, total, start_time, errors=0):
    """In b√°o c√°o ti·∫øn tr√¨nh chi ti·∫øt"""
    elapsed = time.time() - start_time
    rate = processed / elapsed if elapsed > 0 else 0
    
    print(f"\n{'='*60}")
    print(f"üìä B√ÅO C√ÅO TI·∫æN TR√åNH:")
    print(f"‚úÖ ƒê√£ x·ª≠ l√Ω: {processed}/{total} ({processed/total*100:.1f}%)")
    print(f"‚ö° T·ªëc ƒë·ªô: {rate:.2f} records/gi√¢y")
    print(f"‚è±Ô∏è Th·ªùi gian ƒë√£ ch·∫°y: {elapsed/3600:.1f} gi·ªù")
    print(f"‚ùå L·ªói: {errors}")
    print(f"‚è∞ {estimate_completion_time(processed, total, start_time)}")
    print(f"{'='*60}\n")

def validate_prompt(prompt):
    """Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa prompt"""
    if not prompt or prompt.strip() == "":
        return False, "Prompt kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng"
    
    if len(prompt.strip()) < 10:
        return False, "Prompt qu√° ng·∫Øn (t·ªëi thi·ªÉu 10 k√Ω t·ª±)"
    
    return True, "Prompt h·ª£p l·ªá"

def clean_text(text):
    """L√†m s·∫°ch text tr∆∞·ªõc khi x·ª≠ l√Ω"""
    if pd.isna(text):
        return ""
    
    text = str(text).strip()
    
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng mong mu·ªën
    text = text.replace('\n\n\n', '\n\n')
    text = text.replace('\t', ' ')
    
    return text

def get_processing_stats(df, result_column):
    """L·∫•y th·ªëng k√™ qu√° tr√¨nh x·ª≠ l√Ω"""
    if result_column not in df.columns:
        return {
            'total': len(df),
            'processed': 0,
            'remaining': len(df),
            'errors': 0
        }
    
    total = len(df)
    
    # T√¨m t·∫•t c·∫£ error patterns
    error_patterns = [
        "L·ªói",           # Vietnamese error
        "Batch error",   # Async batch error
        "failed after",  # Batch failed after X attempts
        "HTTP 429",      # Rate limit error
        "Timeout",       # Timeout error
        "Connection error"  # Connection error
    ]
    
    # T·∫°o mask cho errors
    error_mask = pd.Series([False] * len(df))
    for pattern in error_patterns:
        pattern_mask = df[result_column].astype(str).str.contains(pattern, case=False, na=False)
        error_mask = error_mask | pattern_mask
    
    errors = error_mask.sum()
    
    # Processed = c√≥ data v√† kh√¥ng ph·∫£i error
    has_data_mask = df[result_column].notna() & (df[result_column] != "")
    processed = (has_data_mask & ~error_mask).sum()
    remaining = total - has_data_mask.sum()  # Ch∆∞a c√≥ data g√¨ c·∫£
    
    return {
        'total': total,
        'processed': processed,  # Th√†nh c√¥ng th·ª±c s·ª±
        'remaining': remaining,  # Ch∆∞a x·ª≠ l√Ω
        'errors': errors        # C√≥ l·ªói
    }

def apply_fine_tuned_prompt_context(text, prompt, context_template):
    """
    Apply prompt context template cho fine-tuned model
    
    Args:
        text: Input text
        prompt: User prompt  
        context_template: Template t·ª´ fine-tuning
        
    Returns:
        str: Formatted prompt theo fine-tuned model
    """
    try:
        # T·∫°o context data
        context_data = {
            'input_data': text,
            'input_text': text,
            'content': text,
            'message': text,
            'context_info': prompt,
            'context_data': prompt,
            'metadata': prompt,
            'additional_context': prompt
        }
        
        # Apply template v·ªõi placeholder replacement
        formatted_prompt = context_template
        
        for key, value in context_data.items():
            placeholder = f"{{{key}}}"
            if placeholder in formatted_prompt:
                formatted_prompt = formatted_prompt.replace(placeholder, str(value))
        
        logger.debug(f"üìù Applied fine-tuned context template")
        return formatted_prompt
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è L·ªói apply context template: {str(e)}, fallback to standard")
        return f"{prompt}\n\nN·ªôi dung c·∫ßn x·ª≠ l√Ω:\n{text}\n\nK·∫øt qu·∫£:"

def load_fine_tuned_models():
    """Load danh s√°ch fine-tuned models t·ª´ registry"""
    try:
        registry_file = "fine_tuned_models.json"
        if os.path.exists(registry_file):
            with open(registry_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data.get('fine_tuned_models', {})
        else:
            return {}
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load fine-tuned models: {str(e)}")
        return {} 