import pandas as pd
import numpy as np
import os
import time
import logging
import json
import re
from datetime import datetime, timedelta
from pathlib import Path
import google.generativeai as genai
import asyncio
import jsonschema
from jsonschema import validate, ValidationError

# Import async processor
try:
    from async_processor import process_data_async
    ASYNC_AVAILABLE = True
except ImportError as e:
    ASYNC_AVAILABLE = False
    print(f"‚ö†Ô∏è Async processing kh√¥ng kh·∫£ d·ª•ng: {e}")

# Import config
from config import (
    MAX_OUTPUT_TOKENS,
    TEMPERATURE, 
    TOP_P,
    TOP_K,
    MAX_RETRIES,
    RETRY_DELAY,
    REQUEST_DELAY,
    SUPPORTED_FORMATS,
    LOG_FILE,
    LOG_LEVEL,
    BATCH_SIZE,
    ENABLE_BATCH_PROCESSING,
    ENABLE_PARALLEL_PROCESSING,
    ENABLE_ASYNC_PROCESSING,
    MAX_CONCURRENT_THREADS,
    THREAD_BATCH_SIZE,
    RATE_LIMIT_DELAY,
    MAX_RETRIES_PER_THREAD,
    THREAD_TIMEOUT,
    CIRCUIT_BREAKER_THRESHOLD,
    GEMINI_API_KEY,
    GEMINI_MODEL_NAME,
    OPENAI_API_KEY,
    OPENAI_MODEL_NAME,
    JSON_OUTPUT_SCHEMA,
    JSON_VALIDATE_SCHEMA,
    JSON_REPAIR_MALFORMED,
    JSON_PARSE_FALLBACK_TO_TEXT
)

def check_openai_availability():
    """Ki·ªÉm tra OpenAI availability ƒë·ªông"""
    try:
        from openai import OpenAI
        return True, OpenAI
    except ImportError:
        try:
            import openai
            if hasattr(openai, 'OpenAI'):
                return True, openai.OpenAI
            else:
                return False, None
        except ImportError:
            return False, None

# Ki·ªÉm tra ban ƒë·∫ßu
OPENAI_AVAILABLE, OpenAI = check_openai_availability()
from tqdm import tqdm
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
import traceback

def setup_logging():
    """Thi·∫øt l·∫≠p logging cho ·ª©ng d·ª•ng"""
    logging.basicConfig(
        level=getattr(logging, LOG_LEVEL),
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(LOG_FILE, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

def initialize_gemini(api_key, model_name, fine_tuned_model_info=None):
    """Kh·ªüi t·∫°o Gemini API v·ªõi c·∫•u h√¨nh (h·ªó tr·ª£ fine-tuned models)"""
    try:
        genai.configure(api_key=api_key)
        
        # N·∫øu l√† fine-tuned model, s·ª≠ d·ª•ng model_id
        if fine_tuned_model_info and fine_tuned_model_info.get('model_id'):
            actual_model_name = fine_tuned_model_info['model_id']
            logger.info(f"üéØ S·ª≠ d·ª•ng fine-tuned model: {actual_model_name}")
        else:
            actual_model_name = model_name
            logger.info(f"ü§ñ S·ª≠ d·ª•ng standard model: {actual_model_name}")
        
        model = genai.GenerativeModel(
            model_name=actual_model_name,
            generation_config={
                "max_output_tokens": MAX_OUTPUT_TOKENS,
                "temperature": TEMPERATURE,
                "top_p": TOP_P,
                "top_k": TOP_K,
            }
        )
        
        # Attach fine-tuned model info n·∫øu c√≥
        if fine_tuned_model_info:
            model._fine_tuned_info = fine_tuned_model_info
        
        # Attach API provider info
        model._api_provider = 'gemini'
        model._api_key = api_key
        model._model_name = actual_model_name
        
        logger.info(f"‚úÖ ƒê√£ kh·ªüi t·∫°o Gemini model: {actual_model_name}")
        return model
    except Exception as e:
        logger.error(f"‚ùå L·ªói kh·ªüi t·∫°o Gemini: {str(e)}")
        return None

def initialize_openai(api_key, model_name):
    """Kh·ªüi t·∫°o OpenAI API v·ªõi c·∫•u h√¨nh"""
    # Ki·ªÉm tra l·∫°i ƒë·ªông
    available, openai_class = check_openai_availability()
    if not available or openai_class is None:
        raise ImportError("OpenAI package kh√¥ng c√≥ s·∫µn ho·∫∑c version kh√¥ng t∆∞∆°ng th√≠ch. Vui l√≤ng c√†i ƒë·∫∑t: pip install openai>=1.0.0")
    
    try:
        client = openai_class(api_key=api_key)
        
        # Test connection
        test_response = client.models.list()
        
        # Create a wrapper object that behaves like Gemini model
        class OpenAIModelWrapper:
            def __init__(self, client, model_name):
                self.client = client
                self.model_name = model_name
                self._api_provider = 'openai'
                self._fine_tuned_info = None
                self._api_key = client.api_key
                self._model_name = model_name
            
            def generate_content(self, prompt):
                try:
                    response = self.client.chat.completions.create(
                        model=self.model_name,
                        messages=[
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=MAX_OUTPUT_TOKENS,
                        temperature=TEMPERATURE,
                        top_p=TOP_P
                    )
                    
                    if response.choices and response.choices[0].message:
                        # Create response object similar to Gemini
                        class ResponseWrapper:
                            def __init__(self, text):
                                self.text = text
                        
                        return ResponseWrapper(response.choices[0].message.content)
                    else:
                        return None
                        
                except Exception as e:
                    raise Exception(f"OpenAI API error: {str(e)}")
        
        model = OpenAIModelWrapper(client, model_name)
        logger.info(f"‚úÖ ƒê√£ kh·ªüi t·∫°o OpenAI model: {model_name}")
        return model
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói kh·ªüi t·∫°o OpenAI: {str(e)}")
        return None

def initialize_ai_model(api_provider, api_key, model_name, fine_tuned_model_info=None):
    """Kh·ªüi t·∫°o AI model d·ª±a tr√™n provider"""
    if api_provider == 'gemini':
        model = initialize_gemini(api_key, model_name, fine_tuned_model_info)
        if model:
            # Th√™m th√¥ng tin API provider cho async processing
            model._api_provider = "gemini"
        return model
    elif api_provider == 'openai':
        # Ki·ªÉm tra l·∫°i ƒë·ªông
        available, _ = check_openai_availability()
        if not available:
            logger.error("‚ùå OpenAI package kh√¥ng c√≥ s·∫µn. Vui l√≤ng c√†i ƒë·∫∑t: pip install openai>=1.0.0")
            return None
        model = initialize_openai(api_key, model_name)
        if model:
            # Th√™m th√¥ng tin API provider cho async processing
            model._api_provider = "openai"
        return model
    else:
        logger.error(f"‚ùå API provider kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {api_provider}")
        return None

def validate_file_path(file_path):
    """Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa ƒë∆∞·ªùng d·∫´n file"""
    if not file_path:
        return False, "ƒê∆∞·ªùng d·∫´n file kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng"
    
    path = Path(file_path)
    
    # Ki·ªÉm tra file c√≥ t·ªìn t·∫°i
    if not path.exists():
        return False, f"File kh√¥ng t·ªìn t·∫°i: {file_path}"
    
    # Ki·ªÉm tra ƒë·ªãnh d·∫°ng file
    if path.suffix.lower() not in SUPPORTED_FORMATS:
        return False, f"ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. Ch·ªâ h·ªó tr·ª£: {', '.join(SUPPORTED_FORMATS)}"
    
    return True, "File h·ª£p l·ªá"

def load_data(file_path):
    """Load d·ªØ li·ªáu t·ª´ file Excel ho·∫∑c CSV"""
    try:
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path)
        elif file_ext == '.csv':
            # Th·ª≠ nhi·ªÅu encoding ƒë·ªÉ tr√°nh l·ªói
            encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']
            df = None
            for encoding in encodings:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue
            
            if df is None:
                raise Exception("Kh√¥ng th·ªÉ ƒë·ªçc file CSV v·ªõi c√°c encoding th√¥ng d·ª•ng")
        else:
            raise Exception(f"ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {file_ext}")
        
        logger.info(f"‚úÖ ƒê√£ load file: {file_path} ({len(df)} records)")
        return df
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói load file: {str(e)}")
        return None

def save_data(df, file_path):
    """L∆∞u d·ªØ li·ªáu ra file Excel ho·∫∑c CSV"""
    try:
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext in ['.xlsx', '.xls']:
            df.to_excel(file_path, index=False)
        elif file_ext == '.csv':
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
        else:
            raise Exception(f"ƒê·ªãnh d·∫°ng file output kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {file_ext}")
        
        logger.info(f"‚úÖ ƒê√£ l∆∞u file: {file_path}")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói l∆∞u file: {str(e)}")
        return False

def detect_message_column(df):
    """T·ª± ƒë·ªông ph√°t hi·ªán c·ªôt ch·ª©a message"""
    possible_columns = ['MESSAGE']
    
    for col in possible_columns:
        if col in df.columns:
            return col
    
    # N·∫øu kh√¥ng t√¨m th·∫•y, tr·∫£ v·ªÅ c·ªôt ƒë·∫ßu ti√™n c√≥ ki·ªÉu text
    for col in df.columns:
        if df[col].dtype == 'object':
            return col
    
    return None

def generate_output_filename(input_file, output_format=None):
    """T·∫°o t√™n file output d·ª±a tr√™n file input v√† ƒë·ªãnh d·∫°ng output"""
    input_path = Path(input_file)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if output_format == 'csv':
        output_name = f"{input_path.stem}_ai_result_{timestamp}.csv"
    else:
        output_name = f"{input_path.stem}_ai_result_{timestamp}.xlsx"
    return str(input_path.parent / output_name)

def generate_checkpoint_filename(input_file, output_format=None):
    """T·∫°o t√™n file checkpoint d·ª±a tr√™n file input v√† ƒë·ªãnh d·∫°ng output"""
    input_path = Path(input_file)
    if output_format == 'csv':
        checkpoint_name = f"{input_path.stem}_checkpoint.csv"
    else:
        checkpoint_name = f"{input_path.stem}_checkpoint.xlsx"
    return str(input_path.parent / checkpoint_name)

def load_checkpoint(checkpoint_file):
    """Load checkpoint n·∫øu t·ªìn t·∫°i"""
    try:
        if os.path.exists(checkpoint_file):
            df = load_data(checkpoint_file)
            if df is not None:
                logger.info(f"üîÑ ƒê√£ load checkpoint: {checkpoint_file}")
                return df
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è L·ªói load checkpoint: {str(e)}")
    
    return None

def save_checkpoint(df, checkpoint_file):
    """L∆∞u checkpoint"""
    try:
        save_data(df, checkpoint_file)
        logger.info(f"üíæ ƒê√£ l∆∞u checkpoint: {checkpoint_file}")
    except Exception as e:
        logger.error(f"‚ùå L·ªói l∆∞u checkpoint: {str(e)}")

def process_text_with_ai(model, text, prompt, max_retries=MAX_RETRIES):
    """X·ª≠ l√Ω text v·ªõi AI model (h·ªó tr·ª£ fine-tuned models v·ªõi prompt context)"""
    for attempt in range(max_retries):
        try:
            # Ki·ªÉm tra n·∫øu l√† fine-tuned model v·ªõi prompt context
            if hasattr(model, '_fine_tuned_info') and model._fine_tuned_info:
                fine_tuned_info = model._fine_tuned_info
                
                # N·∫øu model c√≥ prompt context, s·ª≠ d·ª•ng format ƒë√£ training
                if fine_tuned_info.get('requires_context', False):
                    prompt_context = fine_tuned_info.get('prompt_context', {})
                    context_template = prompt_context.get('template', '')
                    
                    if context_template:
                        # Apply prompt context template
                        full_prompt = apply_fine_tuned_prompt_context(
                            text, prompt, context_template
                        )
                        logger.debug(f"üéØ S·ª≠ d·ª•ng fine-tuned prompt context")
                    else:
                        full_prompt = f"{prompt}\n\nN·ªôi dung c·∫ßn x·ª≠ l√Ω:\n{text}\n\nK·∫øt qu·∫£:"
                else:
                    full_prompt = f"{prompt}\n\nN·ªôi dung c·∫ßn x·ª≠ l√Ω:\n{text}\n\nK·∫øt qu·∫£:"
            else:
                # Standard model processing
                full_prompt = f"{prompt}\n\nN·ªôi dung c·∫ßn x·ª≠ l√Ω:\n{text}\n\nK·∫øt qu·∫£:"
            
            # G·ªçi API
            response = model.generate_content(full_prompt)
            
            # Delay ƒë·ªÉ tr√°nh rate limit
            time.sleep(REQUEST_DELAY)
            
            if response and response.text:
                result = response.text.strip()
                logger.debug(f"‚úÖ X·ª≠ l√Ω th√†nh c√¥ng: {text[:50]}...")
                return result
            else:
                raise Exception("Kh√¥ng nh·∫≠n ƒë∆∞·ª£c response t·ª´ AI")
                
        except Exception as e:
            error_msg = str(e)
            
            # X·ª≠ l√Ω l·ªói rate limit (cho c·∫£ Gemini v√† OpenAI)
            if ("429" in error_msg or "quota" in error_msg.lower() or 
                "rate_limit" in error_msg.lower() or "too_many_requests" in error_msg.lower()):
                if attempt < max_retries - 1:
                    wait_time = RETRY_DELAY * (attempt + 1)  # TƒÉng d·∫ßn th·ªùi gian ch·ªù
                    logger.warning(f"‚è≥ Rate limit reached. Ch·ªù {wait_time} gi√¢y...")
                    time.sleep(wait_time)
                    continue
            
            # X·ª≠ l√Ω l·ªói kh√°c
            logger.error(f"‚ùå L·ªói x·ª≠ l√Ω text (attempt {attempt + 1}): {error_msg}")
            
            if attempt < max_retries - 1:
                time.sleep(5)  # Ch·ªù ng·∫Øn tr∆∞·ªõc khi th·ª≠ l·∫°i
                continue
    
    return f"L·ªói x·ª≠ l√Ω sau {max_retries} l·∫ßn th·ª≠"

def process_multicolumn_with_ai(model, row_data, column_names, prompt, max_retries=MAX_RETRIES):
    """X·ª≠ l√Ω nhi·ªÅu c·ªôt v·ªõi AI model"""
    try:
        # T·∫°o c·∫•u tr√∫c d·ªØ li·ªáu cho prompt
        data_structure = "\n".join([
            f"{i+1}. {col_name}: {clean_text(str(row_data.get(col_name, 'N/A')))}"
            for i, col_name in enumerate(column_names)
        ])
        
        # T·∫°o ph·∫ßn ƒë·ªãnh nghƒ©a c·ªôt cho prompt
        column_definitions = "\n".join([
            f"- C·ªôt {i+1} ({col_name}): {_get_column_description(col_name)}"
            for i, col_name in enumerate(column_names)
        ])
        
        # T·∫°o prompt ƒë·∫ßy ƒë·ªß v·ªõi ƒë·ªãnh nghƒ©a c·ªôt
        full_prompt = f"""
{prompt}

TH√îNG TIN C√ÅC C·ªòT:
{column_definitions}

D·ªÆ LI·ªÜU C·∫¶N X·ª¨ L√ù:
{data_structure}

K·∫øt qu·∫£:"""
        
        # G·ªçi h√†m x·ª≠ l√Ω AI
        return process_text_with_ai(model, "", full_prompt, max_retries)
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói x·ª≠ l√Ω multi-column: {str(e)}")
        return f"L·ªói x·ª≠ l√Ω multi-column: {str(e)}"

def _get_column_description(column_name):
    """T·∫°o m√¥ t·∫£ cho c·ªôt d·ª±a tr√™n t√™n c·ªôt"""
    return column_name

def process_batch_with_ai(model, batch_data, prompt, max_retries=MAX_RETRIES):
    """X·ª≠ l√Ω batch data v·ªõi AI model - single column mode"""
    try:
        # T·∫°o prompt cho batch processing
        batch_items = []
        for i, item in enumerate(batch_data, 1):
            cleaned_text = clean_text(str(item))
            batch_items.append(f"[{i}] {cleaned_text}")
        
        batch_content = "\n\n".join(batch_items)
        
        # T·∫°o prompt ƒë·∫ßy ƒë·ªß cho batch
        full_prompt = f"""{prompt}

H∆Ø·ªöNG D·∫™N BATCH PROCESSING:
- X·ª≠ l√Ω {len(batch_data)} m·ª•c d·ªØ li·ªáu d∆∞·ªõi ƒë√¢y
- Tr·∫£ v·ªÅ k·∫øt qu·∫£ theo th·ª© t·ª± t∆∞∆°ng ·ª©ng
- Format: [1] K·∫øt qu·∫£ 1\n[2] K·∫øt qu·∫£ 2\n[3] K·∫øt qu·∫£ 3...

D·ªÆ LI·ªÜU C·∫¶N X·ª¨ L√ù:
{batch_content}

K·∫æT QU·∫¢:"""

        # G·ªçi AI
        for attempt in range(max_retries):
            try:
                response = model.generate_content(full_prompt)
                
                # Delay ƒë·ªÉ tr√°nh rate limit  
                time.sleep(REQUEST_DELAY)
                
                if response and response.text:
                    # Parse k·∫øt qu·∫£ batch
                    results = parse_batch_results(response.text.strip(), len(batch_data))
                    logger.debug(f"‚úÖ X·ª≠ l√Ω batch th√†nh c√¥ng: {len(results)} results")
                    return results
                else:
                    raise Exception("Kh√¥ng nh·∫≠n ƒë∆∞·ª£c response t·ª´ AI")
                    
            except Exception as e:
                error_msg = str(e)
                
                # X·ª≠ l√Ω l·ªói rate limit (cho c·∫£ Gemini v√† OpenAI)
                if ("429" in error_msg or "quota" in error_msg.lower() or 
                    "rate_limit" in error_msg.lower() or "too_many_requests" in error_msg.lower()):
                    if attempt < max_retries - 1:
                        wait_time = RETRY_DELAY * (attempt + 1)
                        logger.warning(f"‚è≥ Rate limit reached. Ch·ªù {wait_time} gi√¢y...")
                        time.sleep(wait_time)
                        continue
                
                # X·ª≠ l√Ω l·ªói kh√°c
                logger.error(f"‚ùå L·ªói x·ª≠ l√Ω batch (attempt {attempt + 1}): {error_msg}")
                
                if attempt < max_retries - 1:
                    time.sleep(5)
                    continue
        
        # N·∫øu batch th·∫•t b·∫°i, fallback v·ªÅ single processing
        logger.warning("‚ö†Ô∏è Batch processing th·∫•t b·∫°i, fallback v·ªÅ single processing")
        results = []
        for item in batch_data:
            result = process_text_with_ai(model, str(item), prompt, max_retries)
            results.append(result)
        return results
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói x·ª≠ l√Ω batch: {str(e)}")
        # Fallback v·ªÅ single processing
        results = []
        for item in batch_data:
            result = process_text_with_ai(model, str(item), prompt, max_retries)
            results.append(result)
        return results

def process_multicolumn_batch_with_ai(model, batch_rows, column_names, prompt, max_retries=MAX_RETRIES):
    """X·ª≠ l√Ω batch data v·ªõi AI model - multi-column mode"""
    try:
        # T·∫°o prompt cho multi-column batch processing
        batch_items = []
        
        # T·∫°o ph·∫ßn ƒë·ªãnh nghƒ©a c·ªôt
        column_definitions = "\n".join([
            f"- C·ªôt {i+1} ({col_name}): {_get_column_description(col_name)}"
            for i, col_name in enumerate(column_names)
        ])
        
        # T·∫°o d·ªØ li·ªáu batch
        for i, row_data in enumerate(batch_rows, 1):
            data_structure = "\n".join([
                f"  {j+1}. {col_name}: {clean_text(str(row_data.get(col_name, 'N/A')))}"
                for j, col_name in enumerate(column_names)
            ])
            batch_items.append(f"[{i}]\n{data_structure}")
        
        batch_content = "\n\n".join(batch_items)
        
        # T·∫°o prompt ƒë·∫ßy ƒë·ªß cho multi-column batch
        full_prompt = f"""{prompt}

TH√îNG TIN C√ÅC C·ªòT:
{column_definitions}

H∆Ø·ªöNG D·∫™N BATCH PROCESSING:
- X·ª≠ l√Ω {len(batch_rows)} m·ª•c d·ªØ li·ªáu d∆∞·ªõi ƒë√¢y
- M·ªói m·ª•c c√≥ {len(column_names)} c·ªôt th√¥ng tin
- Tr·∫£ v·ªÅ k·∫øt qu·∫£ theo th·ª© t·ª± t∆∞∆°ng ·ª©ng
- Format: [1] K·∫øt qu·∫£ 1\n[2] K·∫øt qu·∫£ 2\n[3] K·∫øt qu·∫£ 3...

D·ªÆ LI·ªÜU C·∫¶N X·ª¨ L√ù:
{batch_content}

K·∫æT QU·∫¢:"""

        # G·ªçi AI
        for attempt in range(max_retries):
            try:
                response = model.generate_content(full_prompt)
                
                # Delay ƒë·ªÉ tr√°nh rate limit
                time.sleep(REQUEST_DELAY)
                
                if response and response.text:
                    # Parse k·∫øt qu·∫£ batch
                    results = parse_batch_results(response.text.strip(), len(batch_rows))
                    logger.debug(f"‚úÖ X·ª≠ l√Ω multi-column batch th√†nh c√¥ng: {len(results)} results")
                    return results
                else:
                    raise Exception("Kh√¥ng nh·∫≠n ƒë∆∞·ª£c response t·ª´ AI")
                    
            except Exception as e:
                error_msg = str(e)
                
                # X·ª≠ l√Ω l·ªói rate limit (cho c·∫£ Gemini v√† OpenAI)
                if ("429" in error_msg or "quota" in error_msg.lower() or 
                    "rate_limit" in error_msg.lower() or "too_many_requests" in error_msg.lower()):
                    if attempt < max_retries - 1:
                        wait_time = RETRY_DELAY * (attempt + 1)
                        logger.warning(f"‚è≥ Rate limit reached. Ch·ªù {wait_time} gi√¢y...")
                        time.sleep(wait_time)
                        continue
                
                # X·ª≠ l√Ω l·ªói kh√°c
                logger.error(f"‚ùå L·ªói x·ª≠ l√Ω multi-column batch (attempt {attempt + 1}): {error_msg}")
                
                if attempt < max_retries - 1:
                    time.sleep(5)
                    continue
        
        # N·∫øu batch th·∫•t b·∫°i, fallback v·ªÅ single processing
        logger.warning("‚ö†Ô∏è Multi-column batch processing th·∫•t b·∫°i, fallback v·ªÅ single processing")
        results = []
        for row_data in batch_rows:
            result = process_multicolumn_with_ai(model, row_data, column_names, prompt, max_retries)
            results.append(result)
        return results
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói x·ª≠ l√Ω multi-column batch: {str(e)}")
        # Fallback v·ªÅ single processing
        results = []
        for row_data in batch_rows:
            result = process_multicolumn_with_ai(model, row_data, column_names, prompt, max_retries)
            results.append(result)
        return results

def parse_batch_results(response_text, expected_count):
    """Parse k·∫øt qu·∫£ t·ª´ batch response"""
    try:
        results = []
        lines = response_text.split('\n')
        
        current_result = ""
        current_index = 0
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Ki·ªÉm tra n·∫øu l√† d√≤ng b·∫Øt ƒë·∫ßu v·ªõi [s·ªë]
            if line.startswith('[') and ']' in line:
                # L∆∞u k·∫øt qu·∫£ tr∆∞·ªõc ƒë√≥ n·∫øu c√≥
                if current_result and current_index > 0:
                    results.append(current_result.strip())
                
                # B·∫Øt ƒë·∫ßu k·∫øt qu·∫£ m·ªõi
                bracket_end = line.find(']')
                try:
                    index = int(line[1:bracket_end])
                    current_index = index
                    current_result = line[bracket_end + 1:].strip()
                except ValueError:
                    # N·∫øu kh√¥ng parse ƒë∆∞·ª£c s·ªë, coi nh∆∞ l√† n·ªôi dung
                    current_result += " " + line
            else:
                # Ti·∫øp t·ª•c n·ªôi dung c·ªßa k·∫øt qu·∫£ hi·ªán t·∫°i
                current_result += " " + line
        
        # L∆∞u k·∫øt qu·∫£ cu·ªëi c√πng
        if current_result and current_index > 0:
            results.append(current_result.strip())
        
        # ƒê·∫£m b·∫£o s·ªë l∆∞·ª£ng k·∫øt qu·∫£ ƒë√∫ng
        while len(results) < expected_count:
            results.append("L·ªói parse k·∫øt qu·∫£")
        
        # C·∫Øt b·ªõt n·∫øu c√≥ qu√° nhi·ªÅu k·∫øt qu·∫£
        results = results[:expected_count]
        
        logger.debug(f"‚úÖ Parse batch results: {len(results)}/{expected_count}")
        return results
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói parse batch results: {str(e)}")
        # Fallback: tr·∫£ v·ªÅ response nguy√™n cho t·∫•t c·∫£
        return [response_text] * expected_count

# ===========================
# PARALLEL PROCESSING FUNCTIONS
# ===========================

class CircuitBreaker:
    """Circuit breaker pattern cho parallel processing"""
    def __init__(self, threshold=CIRCUIT_BREAKER_THRESHOLD):
        self.threshold = threshold
        self.failure_count = 0
        self.last_failure_time = None
        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN
        self.reset_timeout = 60  # seconds
        self._lock = threading.Lock()
    
    def call(self, func, *args, **kwargs):
        """G·ªçi function v·ªõi circuit breaker protection"""
        with self._lock:
            if self.state == 'OPEN':
                if time.time() - self.last_failure_time > self.reset_timeout:
                    self.state = 'HALF_OPEN'
                else:
                    raise Exception("Circuit breaker is OPEN")
            
            try:
                result = func(*args, **kwargs)
                if self.state == 'HALF_OPEN':
                    self.state = 'CLOSED'
                    self.failure_count = 0
                return result
            
            except Exception as e:
                self.failure_count += 1
                self.last_failure_time = time.time()
                
                if self.failure_count >= self.threshold:
                    self.state = 'OPEN'
                    logger.warning(f"üî¥ Circuit breaker OPEN - {self.failure_count} failures")
                
                raise e

def process_parallel_batch_worker(thread_id, model, batch_data, column_names, prompt, is_multicolumn, circuit_breaker, progress_queue):
    """Worker function cho parallel processing"""
    try:
        logger.info(f"üöÄ Thread {thread_id} b·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(batch_data)} items")
        
        # Delay staggered ƒë·ªÉ tr√°nh rate limit
        time.sleep(thread_id * RATE_LIMIT_DELAY)
        
        # X·ª≠ l√Ω v·ªõi circuit breaker protection
        if is_multicolumn:
            results = circuit_breaker.call(
                process_multicolumn_batch_with_ai,
                model, batch_data, column_names, prompt, MAX_RETRIES_PER_THREAD
            )
        else:
            results = circuit_breaker.call(
                process_batch_with_ai,
                model, batch_data, prompt, MAX_RETRIES_PER_THREAD
            )
        
        # Report progress
        progress_queue.put(('success', thread_id, len(batch_data)))
        logger.info(f"‚úÖ Thread {thread_id} ho√†n th√†nh th√†nh c√¥ng")
        
        return results
        
    except Exception as e:
        error_msg = f"Thread {thread_id} l·ªói: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        logger.debug(traceback.format_exc())
        
        # Report error
        progress_queue.put(('error', thread_id, str(e)))
        
        # Fallback v·ªÅ single processing cho batch n√†y
        results = []
        try:
            if is_multicolumn:
                for row_data in batch_data:
                    result = process_multicolumn_with_ai(model, row_data, column_names, prompt, MAX_RETRIES_PER_THREAD)
                    results.append(result)
            else:
                for item in batch_data:
                    result = process_text_with_ai(model, str(item), prompt, MAX_RETRIES_PER_THREAD)
                    results.append(result)
        except Exception as fallback_error:
            logger.error(f"‚ùå Thread {thread_id} fallback c≈©ng th·∫•t b·∫°i: {str(fallback_error)}")
            results = [f"L·ªói x·ª≠ l√Ω: {str(e)}"] * len(batch_data)
        
        return results

def process_data_with_async(model, data, column_names, prompt, is_multicolumn=False, checkpoint_callback=None, checkpoint_interval=None, use_json=False):
    """X·ª≠ l√Ω d·ªØ li·ªáu v·ªõi Async Processing (thay th·∫ø parallel processing) v·ªõi checkpoint support"""
    # Import config t·∫°i runtime ƒë·ªÉ tr√°nh l·ªói circular import
    try:
        from config import ENABLE_ASYNC_PROCESSING
    except ImportError as e:
        logger.error(f"‚ùå Kh√¥ng th·ªÉ import config: {e}")
        return process_data_batch_only(model, data, column_names, prompt, is_multicolumn)
    
    if not ASYNC_AVAILABLE:
        logger.warning("‚ö†Ô∏è Async processing kh√¥ng kh·∫£ d·ª•ng, fallback v·ªÅ batch processing")
        return process_data_batch_only(model, data, column_names, prompt, is_multicolumn)
    
    if not ENABLE_ASYNC_PROCESSING:
        logger.info("üîÑ Async processing b·ªã t·∫Øt, chuy·ªÉn v·ªÅ batch processing")
        return process_data_batch_only(model, data, column_names, prompt, is_multicolumn)
    
    try:
        total_items = len(data)
        logger.info(f"üöÄ Starting async processing: {total_items} items")
        
        # Extract API info t·ª´ model - L·∫§Y TR·ª∞C TI·∫æP T·ª™ MODEL OBJECT
        api_provider = getattr(model, '_api_provider', 'gemini')
        
        # L·∫•y API key v√† model name t·ª´ model object (ƒë√£ ƒë∆∞·ª£c set khi initialize)
        if hasattr(model, '_api_key') and hasattr(model, '_model_name'):
            api_key = model._api_key
            model_name = model._model_name
            logger.info(f"üîë S·ª≠ d·ª•ng API key t·ª´ model: {api_provider} - {model_name}")
        else:
            # Kh√¥ng c√≥ API key trong model, fallback v·ªÅ batch processing
            logger.warning("‚ö†Ô∏è Model kh√¥ng c√≥ API key cho async processing, fallback v·ªÅ batch processing")
            return process_data_batch_only(model, data, column_names, prompt, is_multicolumn)
        
        # Prepare data cho async processing
        if is_multicolumn:
            # Convert DataFrame rows to dict cho multicolumn
            data_for_async = []
            for _, row in data.iterrows() if hasattr(data, 'iterrows') else enumerate(data):
                if hasattr(data, 'iterrows'):
                    data_for_async.append(row.to_dict())
                else:
                    # N·∫øu data l√† list of dicts
                    data_for_async.append(row)
        else:
            # Single column data
            if hasattr(data, 'tolist'):
                data_for_async = data.tolist()
            else:
                data_for_async = list(data)
        
        # Run async processing v·ªõi checkpoint support
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            results = loop.run_until_complete(
                process_data_async(
                    api_provider=api_provider,
                    api_key=api_key,
                    model_name=model_name,
                    data=data_for_async,
                    prompt_template=prompt,
                    is_multicolumn=is_multicolumn,
                    column_names=column_names,
                    checkpoint_callback=checkpoint_callback,
                    checkpoint_interval=checkpoint_interval,
                    use_json=use_json
                )
            )
            
            logger.info(f"‚úÖ Async processing completed: {len(results)} results")
            return results
            
        finally:
            loop.close()
        
    except Exception as e:
        logger.error(f"‚ùå Async processing failed: {str(e)}")
        logger.debug(traceback.format_exc())
        logger.info("üîÑ Fallback to batch processing")
        return process_data_batch_only(model, data, column_names, prompt, is_multicolumn)

def process_data_parallel(model, data, column_names, prompt, is_multicolumn=False):
    """X·ª≠ l√Ω d·ªØ li·ªáu v·ªõi parallel processing (DEPRECATED - s·ª≠ d·ª•ng async thay th·∫ø)"""
    logger.warning("‚ö†Ô∏è process_data_parallel is deprecated, using async processing instead")
    return process_data_with_async(model, data, column_names, prompt, is_multicolumn)

def process_data_batch_only(model, data, column_names, prompt, is_multicolumn=False):
    """X·ª≠ l√Ω d·ªØ li·ªáu ch·ªâ v·ªõi batch processing (kh√¥ng parallel/async)"""
    try:
        total_items = len(data)
        batch_size = BATCH_SIZE
        all_results = []
        
        logger.info(f"üì¶ Batch processing: {total_items} items v·ªõi batch size {batch_size}")
        
        with tqdm(total=total_items, desc="üîÑ Batch Processing") as pbar:
            for i in range(0, total_items, batch_size):
                batch = data[i:i+batch_size]
                
                if is_multicolumn:
                    results = process_multicolumn_batch_with_ai(model, batch, column_names, prompt)
                else:
                    results = process_batch_with_ai(model, batch, prompt)
                
                all_results.extend(results)
                pbar.update(len(batch))
        
        return all_results
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói batch processing: {str(e)}")
        # Fallback v·ªÅ single processing
        results = []
        for item in data:
            if is_multicolumn:
                result = process_multicolumn_with_ai(model, item, column_names, prompt)
            else:
                result = process_text_with_ai(model, str(item), prompt)
            results.append(result)
        return results

def estimate_completion_time(processed, total, start_time):
    """∆Ø·ªõc t√≠nh th·ªùi gian ho√†n th√†nh"""
    if processed == 0:
        return "Kh√¥ng th·ªÉ ∆∞·ªõc t√≠nh"
    
    elapsed = time.time() - start_time
    rate = processed / elapsed  # records per second
    remaining = total - processed
    
    if rate > 0:
        eta_seconds = remaining / rate
        eta = datetime.now() + timedelta(seconds=eta_seconds)
        return f"D·ª± ki·∫øn ho√†n th√†nh: {eta.strftime('%H:%M:%S %d/%m/%Y')}"
    
    return "Kh√¥ng th·ªÉ ∆∞·ªõc t√≠nh"

def print_progress_summary(processed, total, start_time, errors=0):
    """In b√°o c√°o ti·∫øn tr√¨nh chi ti·∫øt"""
    elapsed = time.time() - start_time
    rate = processed / elapsed if elapsed > 0 else 0
    
    print(f"\n{'='*60}")
    print(f"üìä B√ÅO C√ÅO TI·∫æN TR√åNH:")
    print(f"‚úÖ ƒê√£ x·ª≠ l√Ω: {processed}/{total} ({processed/total*100:.1f}%)")
    print(f"‚ö° T·ªëc ƒë·ªô: {rate:.2f} records/gi√¢y")
    print(f"‚è±Ô∏è Th·ªùi gian ƒë√£ ch·∫°y: {elapsed/3600:.1f} gi·ªù")
    print(f"‚ùå L·ªói: {errors}")
    print(f"‚è∞ {estimate_completion_time(processed, total, start_time)}")
    print(f"{'='*60}\n")

def validate_prompt(prompt):
    """Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa prompt"""
    if not prompt or prompt.strip() == "":
        return False, "Prompt kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng"
    
    if len(prompt.strip()) < 10:
        return False, "Prompt qu√° ng·∫Øn (t·ªëi thi·ªÉu 10 k√Ω t·ª±)"
    
    return True, "Prompt h·ª£p l·ªá"

def clean_text(text):
    """L√†m s·∫°ch text tr∆∞·ªõc khi x·ª≠ l√Ω"""
    if pd.isna(text):
        return ""
    
    text = str(text).strip()
    
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng mong mu·ªën
    text = text.replace('\n\n\n', '\n\n')
    text = text.replace('\t', ' ')
    
    return text

def get_processing_stats(df, result_column):
    """L·∫•y th·ªëng k√™ qu√° tr√¨nh x·ª≠ l√Ω"""
    if result_column not in df.columns:
        return {
            'total': len(df),
            'processed': 0,
            'remaining': len(df),
            'errors': 0
        }
    
    total = len(df)
    
    # T√¨m t·∫•t c·∫£ error patterns
    error_patterns = [
        "L·ªói",           # Vietnamese error
        "Batch error",   # Async batch error
        "failed after",  # Batch failed after X attempts
        "HTTP 429",      # Rate limit error
        "Timeout",       # Timeout error
        "Connection error"  # Connection error
    ]
    
    # T·∫°o mask cho errors
    error_mask = pd.Series([False] * len(df))
    for pattern in error_patterns:
        pattern_mask = df[result_column].astype(str).str.contains(pattern, case=False, na=False)
        error_mask = error_mask | pattern_mask
    
    errors = error_mask.sum()
    
    # Processed = c√≥ data v√† kh√¥ng ph·∫£i error
    has_data_mask = df[result_column].notna() & (df[result_column] != "")
    processed = (has_data_mask & ~error_mask).sum()
    remaining = total - has_data_mask.sum()  # Ch∆∞a c√≥ data g√¨ c·∫£
    
    return {
        'total': total,
        'processed': processed,  # Th√†nh c√¥ng th·ª±c s·ª±
        'remaining': remaining,  # Ch∆∞a x·ª≠ l√Ω
        'errors': errors        # C√≥ l·ªói
    }

def apply_fine_tuned_prompt_context(text, prompt, context_template):
    """
    Apply prompt context template cho fine-tuned model
    
    Args:
        text: Input text
        prompt: User prompt  
        context_template: Template t·ª´ fine-tuning
        
    Returns:
        str: Formatted prompt theo fine-tuned model
    """
    try:
        # T·∫°o context data
        context_data = {
            'input_data': text,
            'input_text': text,
            'content': text,
            'message': text,
            'context_info': prompt,
            'context_data': prompt,
            'metadata': prompt,
            'additional_context': prompt
        }
        
        # Apply template v·ªõi placeholder replacement
        formatted_prompt = context_template
        
        for key, value in context_data.items():
            placeholder = f"{{{key}}}"
            if placeholder in formatted_prompt:
                formatted_prompt = formatted_prompt.replace(placeholder, str(value))
        
        logger.debug(f"üìù Applied fine-tuned context template")
        return formatted_prompt
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è L·ªói apply context template: {str(e)}, fallback to standard")
        return f"{prompt}\n\nN·ªôi dung c·∫ßn x·ª≠ l√Ω:\n{text}\n\nK·∫øt qu·∫£:"

def load_fine_tuned_models():
    """Load danh s√°ch fine-tuned models t·ª´ registry"""
    try:
        registry_file = "fine_tuned_models.json"
        if os.path.exists(registry_file):
            with open(registry_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data.get('fine_tuned_models', {})
        else:
            return {}
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load fine-tuned models: {str(e)}")
        return {}

def check_and_retry_failed_rows(df, result_column, model, column_names, prompt, is_multicolumn=False, max_retry_attempts=2):
    """
    Ki·ªÉm tra v√† x·ª≠ l√Ω l·∫°i c√°c row b·ªã l·ªói tr∆∞·ªõc khi ho√†n th√†nh ETL
    
    Args:
        df: DataFrame ch·ª©a d·ªØ li·ªáu
        result_column: T√™n c·ªôt ch·ª©a k·∫øt qu·∫£ AI
        model: AI model ƒë√£ kh·ªüi t·∫°o
        column_names: Danh s√°ch t√™n c·ªôt (cho multicolumn) ho·∫∑c t√™n c·ªôt message
        prompt: Prompt template
        is_multicolumn: True n·∫øu x·ª≠ l√Ω nhi·ªÅu c·ªôt
        max_retry_attempts: S·ªë l·∫ßn retry t·ªëi ƒëa
        
    Returns:
        dict: Th·ªëng k√™ k·∫øt qu·∫£ retry
    """
    logger.info("üîç Ki·ªÉm tra c√°c row b·ªã l·ªói...")
    
    # T√¨m t·∫•t c·∫£ error patterns
    error_patterns = [
        "L·ªói",           # Vietnamese error
        "Batch error",   # Async batch error  
        "failed after",  # Batch failed after X attempts
        "HTTP 429",      # Rate limit error
        "Timeout",       # Timeout error
        "Connection error",  # Connection error
        "Rate limit",    # Rate limit variations
        "Request failed", # Request failed
        "API error",     # API error
        "Exception",     # General exception
        "Error:",        # Error with colon
        "‚ùå"            # Error emoji
    ]
    
    # T·∫°o mask cho errors
    error_mask = pd.Series([False] * len(df))
    for pattern in error_patterns:
        pattern_mask = df[result_column].astype(str).str.contains(pattern, case=False, na=False)
        error_mask = error_mask | pattern_mask
    
    # T√¨m c√°c row c√≥ l·ªói
    error_indices = df[error_mask].index.tolist()
    
    if not error_indices:
        logger.info("‚úÖ Kh√¥ng c√≥ row n√†o b·ªã l·ªói!")
        return {
            'total_errors': 0,
            'retry_attempted': 0,
            'retry_success': 0,
            'retry_failed': 0
        }
    
    logger.info(f"üî• T√¨m th·∫•y {len(error_indices)} row b·ªã l·ªói, b·∫Øt ƒë·∫ßu retry...")
    
    retry_stats = {
        'total_errors': len(error_indices),
        'retry_attempted': 0,
        'retry_success': 0,
        'retry_failed': 0
    }
    
    # Progress bar cho retry process
    with tqdm(error_indices, desc="üîÑ Retry Failed Rows", ncols=100) as pbar:
        for idx in pbar:
            try:
                retry_stats['retry_attempted'] += 1
                
                # L·∫•y th√¥ng tin row hi·ªán t·∫°i
                current_error = str(df.at[idx, result_column])
                pbar.set_description(f"üîÑ Retry row {idx} (Error: {current_error[:30]}...)")
                
                success = False
                
                # Th·ª≠ retry v·ªõi delay tƒÉng d·∫ßn
                for attempt in range(max_retry_attempts):
                    try:
                        # Delay tƒÉng d·∫ßn: 2s, 5s, 10s
                        if attempt > 0:
                            delay = 2 * (attempt + 1)
                            logger.debug(f"‚è≥ Waiting {delay}s before retry attempt {attempt + 1}")
                            time.sleep(delay)
                        
                        # X·ª≠ l√Ω l·∫°i row n√†y
                        if is_multicolumn:
                            # Multi-column mode
                            row_data = {}
                            has_data = False
                            
                            for col in column_names:
                                if col in df.columns:
                                    value = df.at[idx, col]
                                    row_data[col] = value
                                    if pd.notna(value) and str(value).strip():
                                        has_data = True
                            
                            if not has_data:
                                df.at[idx, result_column] = "Kh√¥ng c√≥ d·ªØ li·ªáu"
                                success = True
                                break
                            
                            # X·ª≠ l√Ω v·ªõi AI multi-column
                            result = process_multicolumn_with_ai(
                                model,
                                row_data,
                                column_names,
                                prompt,
                                max_retries=1  # Gi·∫£m retry trong function ƒë·ªÉ tr√°nh nested retry
                            )
                        else:
                            # Single column mode
                            message_col = column_names if isinstance(column_names, str) else column_names[0]
                            
                            if pd.isna(df.at[idx, message_col]):
                                df.at[idx, result_column] = "Kh√¥ng c√≥ d·ªØ li·ªáu"
                                success = True
                                break
                            
                            # L·∫•y v√† l√†m s·∫°ch text
                            text = clean_text(df.at[idx, message_col])
                            
                            if not text:
                                df.at[idx, result_column] = "Kh√¥ng c√≥ d·ªØ li·ªáu"
                                success = True
                                break
                            
                            # X·ª≠ l√Ω v·ªõi AI
                            result = process_text_with_ai(
                                model, 
                                text, 
                                prompt,
                                max_retries=1  # Gi·∫£m retry trong function
                            )
                        
                        # Ki·ªÉm tra k·∫øt qu·∫£ c√≥ ph·∫£i error kh√¥ng
                        if result and not any(pattern.lower() in str(result).lower() for pattern in error_patterns):
                            df.at[idx, result_column] = result
                            success = True
                            logger.debug(f"‚úÖ Row {idx} retry success")
                            break
                        else:
                            logger.debug(f"‚ö†Ô∏è Row {idx} retry attempt {attempt + 1} still failed: {str(result)[:50]}...")
                            
                    except Exception as retry_error:
                        logger.debug(f"‚ùå Row {idx} retry attempt {attempt + 1} exception: {str(retry_error)}")
                        continue
                
                # C·∫≠p nh·∫≠t stats
                if success:
                    retry_stats['retry_success'] += 1
                    pbar.set_description(f"‚úÖ Row {idx} retry success")
                else:
                    retry_stats['retry_failed'] += 1
                    # Gi·ªØ nguy√™n error message c≈© ho·∫∑c c·∫≠p nh·∫≠t
                    df.at[idx, result_column] = f"Retry failed: {current_error}"
                    pbar.set_description(f"‚ùå Row {idx} retry failed")
                
            except Exception as e:
                retry_stats['retry_failed'] += 1
                logger.error(f"üí• Unexpected error retrying row {idx}: {str(e)}")
                pbar.set_description(f"üí• Row {idx} unexpected error")
    
    # Log k·∫øt qu·∫£ retry
    logger.info(f"üéØ Retry Results:")
    logger.info(f"   - Total errors found: {retry_stats['total_errors']}")
    logger.info(f"   - Retry attempted: {retry_stats['retry_attempted']}")
    logger.info(f"   - Retry success: {retry_stats['retry_success']}")
    logger.info(f"   - Retry failed: {retry_stats['retry_failed']}")
    
    if retry_stats['retry_success'] > 0:
        success_rate = (retry_stats['retry_success'] / retry_stats['retry_attempted']) * 100
        logger.info(f"   - Retry success rate: {success_rate:.1f}%")
    
    return retry_stats 

# ===========================
# JSON PARSING UTILITIES  
# ===========================

def parse_json_response(response_text, schema=None, repair_malformed=True):
    """
    Parse JSON response t·ª´ AI v·ªõi error handling v√† repair
    
    Args:
        response_text: Text response t·ª´ AI
        schema: JSON schema ƒë·ªÉ validate (optional)
        repair_malformed: C√≥ th·ª≠ s·ª≠a JSON l·ªói format kh√¥ng
        
    Returns:
        dict: Parsed JSON object ho·∫∑c None n·∫øu th·∫•t b·∫°i
    """
    # S·ª≠ d·ª•ng config n·∫øu kh√¥ng override
    if repair_malformed is None:
        repair_malformed = JSON_REPAIR_MALFORMED
    
    try:
        # Step 1: Clean v√† extract JSON t·ª´ response
        json_text = extract_json_from_text(response_text)
        
        if not json_text:
            logger.debug("‚ùå Kh√¥ng t√¨m th·∫•y JSON trong response")
            return None
        
        # Step 2: Parse JSON
        try:
            json_obj = json.loads(json_text)
            logger.debug("‚úÖ JSON parse th√†nh c√¥ng")
        except json.JSONDecodeError as e:
            if repair_malformed:
                logger.debug(f"‚ö†Ô∏è JSON parse failed, th·ª≠ repair: {str(e)}")
                repaired_json = repair_json_text(json_text)
                if repaired_json:
                    json_obj = json.loads(repaired_json)
                    logger.debug("‚úÖ JSON repair v√† parse th√†nh c√¥ng")
                else:
                    logger.debug("‚ùå JSON repair th·∫•t b·∫°i")
                    return None
            else:
                logger.debug(f"‚ùå JSON parse failed: {str(e)}")
                return None
        
        # Step 3: Validate schema n·∫øu ƒë∆∞·ª£c b·∫≠t v√† c√≥ schema ph√π h·ª£p
        if JSON_VALIDATE_SCHEMA and schema and _is_compatible_schema(json_obj, schema):
            try:
                validate(instance=json_obj, schema=schema)
                logger.debug("‚úÖ JSON schema validation th√†nh c√¥ng")
            except ValidationError as e:
                logger.debug(f"‚ö†Ô∏è JSON schema validation failed: {str(e)}")
                # Kh√¥ng return None, v·∫´n tr·∫£ v·ªÅ object ƒë·ªÉ caller c√≥ th·ªÉ x·ª≠ l√Ω
        else:
            logger.debug("üîÑ B·ªè qua schema validation (schema kh√¥ng t∆∞∆°ng th√≠ch ho·∫∑c kh√¥ng c√≥)")
        
        return json_obj
        
    except Exception as e:
        logger.debug(f"‚ùå JSON parsing error: {str(e)}")
        return None

def _is_compatible_schema(json_obj, schema):
    """
    Ki·ªÉm tra xem JSON object c√≥ t∆∞∆°ng th√≠ch v·ªõi schema kh√¥ng
    
    Args:
        json_obj: JSON object ƒë√£ parse
        schema: JSON schema
        
    Returns:
        bool: True n·∫øu t∆∞∆°ng th√≠ch
    """
    if not isinstance(json_obj, dict) or not isinstance(schema, dict):
        return False
    
    # Schema c≈© (category, product, service, tag, note_1)
    old_schema_fields = ['category', 'product', 'service', 'tag', 'note_1']
    has_old_fields = any(field in json_obj for field in old_schema_fields)
    
    # Schema m·ªõi (Masterise Group - AI_SACTHAI, AI_TOPICS, etc.)
    new_schema_fields = ['AI_SACTHAI', 'AI_TOPICS', 'AI_DOITUONG', 'AI_THELOAINOIDUNG']
    has_new_fields = any(field in json_obj for field in new_schema_fields)
    
    # Ch·ªâ validate n·∫øu c√≥ old schema fields
    return has_old_fields and not has_new_fields

def extract_json_from_text(text):
    """
    Tr√≠ch xu·∫•t JSON object t·ª´ text response c·ªßa AI
    H·ªó tr·ª£ x·ª≠ l√Ω JSON ƒë∆∞·ª£c b·ªçc trong markdown code blocks
    
    Args:
        text: Text ch·ª©a JSON (c√≥ th·ªÉ c√≥ text kh√°c xung quanh)
        
    Returns:
        str: JSON text ƒë∆∞·ª£c extract ho·∫∑c None
    """
    if not text or not isinstance(text, str):
        return None
    
    text = text.strip()
    
    # Method 1: X·ª≠ l√Ω markdown code blocks (```json ... ```)
    markdown_patterns = [
        r'```json\s*(.*?)\s*```',  # ```json ... ```
        r'```\s*(.*?)\s*```',      # ``` ... ```
        r'`(.*?)`',                # ` ... `
    ]
    
    for pattern in markdown_patterns:
        matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
        if matches:
            # L·∫•y match ƒë·∫ßu ti√™n v√† t√¨m JSON object trong ƒë√≥
            for match in matches:
                json_text = _extract_json_object(match.strip())
                if json_text:
                    logger.debug("‚úÖ T√¨m th·∫•y JSON trong markdown code block")
                    return json_text
    
    # Method 2: T√¨m JSON object tr·ª±c ti·∫øp trong text
    json_text = _extract_json_object(text)
    if json_text:
        logger.debug("‚úÖ T√¨m th·∫•y JSON object tr·ª±c ti·∫øp")
        return json_text
    
    logger.debug("‚ùå Kh√¥ng t√¨m th·∫•y JSON object trong text")
    return None

def _extract_json_object(text):
    """
    Helper function ƒë·ªÉ tr√≠ch xu·∫•t JSON object t·ª´ text
    
    Args:
        text: Text ch·ª©a JSON
        
    Returns:
        str: JSON text ƒë∆∞·ª£c extract ho·∫∑c None
    """
    if not text:
        return None
    
    # T√¨m v·ªã tr√≠ { ƒë·∫ßu ti√™n
    start_idx = text.find('{')
    if start_idx == -1:
        return None
    
    # T√¨m matching } b·∫±ng c√°ch ƒë·∫øm brackets
    bracket_count = 0
    end_idx = -1
    
    for i in range(start_idx, len(text)):
        if text[i] == '{':
            bracket_count += 1
        elif text[i] == '}':
            bracket_count -= 1
            if bracket_count == 0:
                end_idx = i
                break
    
    if end_idx == -1:
        # Kh√¥ng t√¨m th·∫•y matching }, th·ª≠ l·∫•y t·ª´ start ƒë·∫øn cu·ªëi
        json_text = text[start_idx:]
    else:
        json_text = text[start_idx:end_idx + 1]
    
    return json_text.strip()

def repair_json_text(json_text):
    """
    Th·ª≠ s·ª≠a JSON text b·ªã l·ªói format ph·ªï bi·∫øn
    
    Args:
        json_text: JSON text c√≥ th·ªÉ b·ªã l·ªói
        
    Returns:
        str: JSON text ƒë√£ ƒë∆∞·ª£c s·ª≠a ho·∫∑c None n·∫øu kh√¥ng s·ª≠a ƒë∆∞·ª£c
    """
    if not json_text:
        return None
    
    try:
        original = json_text.strip()
        repaired = original
        
        # Repair 1: Th√™m quotes cho keys kh√¥ng c√≥ quotes
        repaired = re.sub(r'(\w+):', r'"\1":', repaired)
        
        # Repair 2: Thay single quotes th√†nh double quotes
        repaired = repaired.replace("'", '"')
        
        # Repair 3: S·ª≠a trailing comma
        repaired = re.sub(r',(\s*[}\]])', r'\1', repaired)
        
        # Repair 4: S·ª≠a null kh√¥ng ƒë√∫ng format
        repaired = re.sub(r'\bnull\b', 'null', repaired, flags=re.IGNORECASE)
        repaired = re.sub(r'\bNone\b', 'null', repaired)
        repaired = re.sub(r'\bundefined\b', 'null', repaired)
        
        # Repair 5: S·ª≠a boolean values
        repaired = re.sub(r'\bTrue\b', 'true', repaired)
        repaired = re.sub(r'\bFalse\b', 'false', repaired)
        
        # Repair 6: Remove comments
        repaired = re.sub(r'//.*$', '', repaired, flags=re.MULTILINE)
        repaired = re.sub(r'/\*.*?\*/', '', repaired, flags=re.DOTALL)
        
        # Test n·∫øu repair th√†nh c√¥ng
        try:
            json.loads(repaired)
            logger.debug("‚úÖ JSON repair th√†nh c√¥ng")
            return repaired
        except json.JSONDecodeError:
            logger.debug("‚ùå JSON repair th·∫•t b·∫°i")
            return None
            
    except Exception as e:
        logger.debug(f"‚ùå JSON repair error: {str(e)}")
        return None

def convert_json_to_text_format(json_obj, delimiter="|"):
    """
    Chuy·ªÉn JSON object th√†nh text format c≈© ƒë·ªÉ t∆∞∆°ng th√≠ch
    Ho·∫∑c tr·∫£ v·ªÅ JSON string n·∫øu schema ph·ª©c t·∫°p
    
    Args:
        json_obj: JSON object ƒë√£ parse
        delimiter: Delimiter ƒë·ªÉ n·ªëi c√°c field
        
    Returns:
        str: Text format theo pattern c≈© ho·∫∑c JSON string
    """
    if not isinstance(json_obj, dict):
        return str(json_obj)
    
    try:
        # Ki·ªÉm tra n·∫øu l√† schema c≈© (category, product, service, tag, note_1)
        old_schema_fields = ['category', 'product', 'service', 'tag', 'note_1']
        if all(field in json_obj for field in old_schema_fields):
            # Theo format: Category|S·∫£n ph·∫©m|Service|Tag|Note 1
            fields = [
                json_obj.get('category') or 'null',
                json_obj.get('product') or 'null', 
                json_obj.get('service') or 'null',
                json_obj.get('tag') or 'null',
                json_obj.get('note_1') or 'null'
            ]
            return delimiter.join(fields)
        
        # N·∫øu l√† schema ph·ª©c t·∫°p (nh∆∞ Masterise Group), tr·∫£ v·ªÅ JSON string
        else:
            import json
            return json.dumps(json_obj, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.debug(f"‚ùå JSON to text conversion error: {str(e)}")
        # Fallback: tr·∫£ v·ªÅ JSON string
        try:
            import json
            return json.dumps(json_obj, ensure_ascii=False)
        except:
            return str(json_obj)

def parse_response_with_fallback(response_text, use_json=False, schema=None):
    """
    Parse response v·ªõi fallback t·ª´ JSON v·ªÅ text parsing
    
    Args:
        response_text: Response text t·ª´ AI
        use_json: C√≥ th·ª≠ parse JSON tr∆∞·ªõc kh√¥ng
        schema: JSON schema ƒë·ªÉ validate
        
    Returns:
        str: Parsed result text
    """
    if not response_text:
        return "Kh√¥ng c√≥ response"
    
    # N·∫øu ƒë∆∞·ª£c b·∫≠t JSON mode, th·ª≠ parse JSON tr∆∞·ªõc
    if use_json:
        json_obj = parse_json_response(response_text, schema)
        
        if json_obj:
            # Convert JSON v·ªÅ text format ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code c≈©
            return convert_json_to_text_format(json_obj)
        elif JSON_PARSE_FALLBACK_TO_TEXT:
            logger.debug("üîÑ JSON parse th·∫•t b·∫°i, fallback v·ªÅ text parsing")
            # C·∫£i thi·ªán: Th·ª≠ extract text c√≥ √Ω nghƒ©a t·ª´ response
            cleaned_text = _extract_meaningful_text(response_text)
            if cleaned_text and cleaned_text != "```json":
                return cleaned_text
            else:
                # N·∫øu v·∫´n kh√¥ng c√≥ text c√≥ √Ω nghƒ©a, tr·∫£ v·ªÅ th√¥ng b√°o l·ªói r√µ r√†ng
                return f"JSON parse failed - Response: {response_text[:100]}..."
        else:
            return f"JSON parse failed: {response_text[:200]}..."
    
    # Fallback v·ªÅ text parsing c≈© ho·∫∑c tr·∫£ v·ªÅ raw text
    return response_text.strip()

def _extract_meaningful_text(response_text):
    """
    Tr√≠ch xu·∫•t text c√≥ √Ω nghƒ©a t·ª´ response, lo·∫°i b·ªè markdown v√† formatting
    
    Args:
        response_text: Response text t·ª´ AI
        
    Returns:
        str: Text c√≥ √Ω nghƒ©a ho·∫∑c None
    """
    if not response_text:
        return None
    
    text = response_text.strip()
    
    # Lo·∫°i b·ªè markdown code blocks
    text = re.sub(r'```json\s*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'```\s*', '', text)
    text = re.sub(r'`\s*', '', text)
    
    # Lo·∫°i b·ªè c√°c pattern kh√¥ng c√≥ √Ω nghƒ©a
    text = re.sub(r'^\s*```json\s*$', '', text, flags=re.IGNORECASE)
    text = re.sub(r'^\s*```\s*$', '', text)
    
    # Lo·∫°i b·ªè c√°c d√≤ng tr·ªëng v√† whitespace th·ª´a
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    text = '\n'.join(lines)
    
    # N·∫øu text qu√° ng·∫Øn ho·∫∑c ch·ªâ ch·ª©a formatting, return None
    if len(text) < 5 or text.lower() in ['json', '```', '```json']:
        return None
    
    return text

# Test utility
def test_json_parsing():
    """Test JSON parsing utilities"""
    test_cases = [
        # Valid JSON
        '{"category": "B√°nh", "product": "ChocoPie", "service": "Review", "tag": "Ch·∫•t l∆∞·ª£ng t·ªët", "note_1": "2"}',
        
        # JSON v·ªõi text xung quanh  
        'K·∫øt qu·∫£ ph√¢n t√≠ch:\n{"category": "N∆∞·ªõc kho√°ng", "product": null, "service": "S·∫£n ph·∫©m", "tag": "H·ªèi th·∫£o lu·∫≠n", "note_1": "2"}\nH·∫øt.',
        
        # JSON v·ªõi l·ªói format
        "{category: 'B√°nh', product: 'ChocoPie', service: 'Review', tag: 'Ch·∫•t l∆∞·ª£ng t·ªët', note_1: '2'}",
        
        # Text kh√¥ng c√≥ JSON
        "Category|ChocoPie|Review|Ch·∫•t l∆∞·ª£ng t·ªët|2"
    ]
    
    print("üß™ Testing JSON parsing utilities...")
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nTest {i}: {test_case[:50]}...")
        
        # Test JSON parsing
        json_result = parse_json_response(test_case)
        print(f"  JSON parse: {json_result}")
        
        # Test v·ªõi fallback
        fallback_result = parse_response_with_fallback(test_case, use_json=True)
        print(f"  With fallback: {fallback_result}")

if __name__ == "__main__":
    test_json_parsing() 